# ML Production Pipeline with Airflow

## ğŸ“Œ Project Overview

This project demonstrates how to build a **production-style Machine Learning pipeline** using:

- Apache Airflow (workflow orchestration)
- Python (ML logic)
- Pandas (data processing)
- Scikit-learn (model training)
- Joblib (model saving)
- Linux (WSL environment)
- Virtual environments (dependency isolation)

The goal was to move from notebook-based ML to **system-level ML engineering**.

---

## ğŸ§  What This Pipeline Does

The pipeline performs the following steps:

1. Extract data from CSV
2. Validate data (fail-fast approach)
3. Train a machine learning model
4. Save versioned model artifacts
5. Orchestrate the entire workflow using Airflow

---

## ğŸ— Project Structure

```
airflow_project/
â”‚
â”œâ”€â”€ airflow_env/               # Python virtual environment
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_data.csv        # Input dataset
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ model_*.pkl            # Versioned trained models
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ ml_pipeline.py         # ML logic (extract, validate, train)
â”‚
â””â”€â”€ ~/airflow/
    â””â”€â”€ dags/
        â””â”€â”€ ml_pipeline.py     # Airflow DAG (orchestration)
```

---

## ğŸ“‚ Dataset Used

File: `data/sample_data.csv`

```
age,salary,target
25,50000,0
30,60000,1
45,80000,1
22,40000,0
```

- `age` â†’ Feature  
- `salary` â†’ Feature  
- `target` â†’ Label  

---

## âš™ï¸ ML Logic (src/ml_pipeline.py)

### 1ï¸âƒ£ Extract

- Reads CSV using Pandas
- Stores intermediate data as `/tmp/data.pkl`

```python
df = pd.read_csv(DATA_PATH)
df.to_pickle("/tmp/data.pkl")
```

Why?  
Airflow tasks are isolated. We pass data via temporary storage.

---

### 2ï¸âƒ£ Validate (Fail-Fast Design)

Checks:

- No null values
- Required columns exist
- Feature columns present

```python
if df.isnull().sum().sum() > 0:
    raise ValueError("Dataset contains null values!")
```

If validation fails:
- Pipeline stops
- Training does not run

---

### 3ï¸âƒ£ Train Model

- Uses Logistic Regression
- Trains on features: age, salary
- Saves model with timestamp

```python
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
model_path = os.path.join(MODEL_DIR, f"model_{timestamp}.pkl")
joblib.dump(model, model_path)
```

Each pipeline run creates:

```
model_20260223_104700.pkl
```

This ensures **model versioning**.

---

## ğŸ”„ Airflow DAG (Orchestration Layer)

The DAG defines execution order:

```
extract_data â†’ validate_data â†’ train_model
```

If validation fails:
- Training never runs
- DAG run fails

Airflow handles:
- Task scheduling
- Retries
- Logging
- Monitoring
- Dependency management

---

## ğŸš€ How to Run the Pipeline

### 1ï¸âƒ£ Activate Virtual Environment

```bash
cd ~/airflow_project
source airflow_env/bin/activate
```

### 2ï¸âƒ£ Start Airflow

```bash
airflow standalone
```

### 3ï¸âƒ£ Open Airflow UI

Open browser:

```
http://localhost:8080
```

### 4ï¸âƒ£ Trigger DAG

- Enable `ml_production_pipeline`
- Click "Trigger DAG"
- Monitor Graph View

---

## ğŸ“¦ Output

After successful run:

```bash
ls ~/airflow_project/models
```

Example output:

```
model_20260223_104624.pkl
model_20260223_104700.pkl
```

Each file represents a versioned trained model.

---

## ğŸ”¥ Key Concepts Learned

### âœ… Virtual Environments
Airflow only sees packages installed inside the active virtual environment.

### âœ… Airflow Home
DAGs must be placed in:

```
$AIRFLOW_HOME/dags
```

### âœ… Separation of Concerns
- Airflow â†’ Orchestration
- src/ â†’ ML logic
- data/ â†’ Input
- models/ â†’ Output

### âœ… Fail-Fast Validation
Data validation stops bad data before training.

### âœ… Model Versioning
Each training run produces a unique timestamped artifact.

---

## ğŸ What This Project Represents

This is no longer notebook experimentation.

It is a structured ML system that:

- Runs tasks in defined order
- Stops on failure
- Saves versioned artifacts
- Can be scheduled daily
- Mimics real production ML pipelines

---

## ğŸ“ˆ Possible Improvements

- Add model evaluation task
- Integrate Great Expectations
- Add MLflow experiment tracking
- Add data drift detection
- Dockerize the project
- Add CI/CD pipeline
- Deploy trained model as API

---

## ğŸ¯ Conclusion

This project marks the transition from:

"Training models in notebooks"

to

"Building production-ready ML pipelines using orchestration."

You now understand how real-world ML systems are structured.
